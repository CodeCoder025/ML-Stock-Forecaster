from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from xgboost import XGBRegressor
def scaleData(trainingData):
    minMaxScaler = MinMaxScaler(feature_range=(0,1))
    scaledTrainData = pd.DataFrame(minMaxScaler.fit_transform(trainingData.drop(columns=["%changeTomorrow"])), columns = [i for i in trainingData.columns.to_list() if i != "%changeTomorrow"])
    scaledTrainData["%changeTomorrow"] = trainingData["%changeTomorrow"]
    return scaledTrainData

def chronologicalSplit(scaledTrainData,x, nRows):
  trainingRows = int(x*nRows)
  trainingSet = scaledTrainData.loc[0:trainingRows]
  testSet = scaledTrainData.loc[trainingRows+1:]
  return trainingSet, testSet

def deployLinearModel(trainData, testData):
  linearRegModel = LinearRegression()
  linearRegModel.fit(trainData.drop(columns = ["%changeTomorrow"]), trainData["%changeTomorrow"])
  linearRegModelPreds =  linearRegModel.predict(testData.drop(columns=["%changeTomorrow"]))
  return linearRegModelPreds, linearRegModel.coef_, linearRegModel.intercept_, compute_RMSE(linearRegModelPreds, testData["%changeTomorrow"])

def deployRandomForest(trainData, testData, bootstrapSize = 0.7, n_dTrees = 150, max_leaf_nodes = 200, max_features = "sqrt"):
    model = RandomForestRegressor(n_jobs=-1,
                                  max_samples = bootstrapSize,
                                  n_estimators = n_dTrees,
                                  max_leaf_nodes = max_leaf_nodes,
                                  max_features = max_features)
    model.fit(trainData.drop(columns=["%changeTomorrow"]),
              trainData["%changeTomorrow"])
    randomForestPreds = model.predict(testData.drop(columns=["%changeTomorrow"]))
    return randomForestPreds, model.feature_importances_,compute_RMSE(randomForestPreds, testData["%changeTomorrow"])

def deployGBM(trainData, testData, n_estimators = 100, learning_rate = 0.6, booster = "gbtree", bootstrapSize = 0.7, max_features = "sqrt"):
    if max_features == "sqrt":
        max_features = int((len(trainData.columns.to_list())-1)**0.5)/(len(trainData.columns.to_list())-1)
    GBM = XGBRegressor(n_estimators = n_estimators,
                      learning_rate = learning_rate,
                      booster = booster,
                      subsample = bootstrapSize,
                      colsample_bytree = max_features)
    GBM.fit(trainData.drop(columns=["%changeTomorrow"]), trainData["%changeTomorrow"])
    GBMPreds = GBM.predict(testData.drop(columns=["%changeTomorrow"]))
    if booster == "gbtree":
        weightsDict = GBM.get_booster().get_score(importance_type = "gain")
    elif booster == "gblinear":
        weightsDict = GBM.get_booster().get_score(importance_type = "weight")
    weightsList = [i for i in weightsDict.values()]
    return GBMPreds, weightsList, compute_RMSE(GBMPreds, testData["%changeTomorrow"])

def compute_RMSE(predicted, actual):
    SE = 0
    for i in range(len(predicted)):
        SE += (predicted[i] - actual.iloc[i])**2
    return (SE/len(predicted))**0.5

def basicDeploy(ticker, startDate = None, endDate=None):
    if startDate and endDate:
        trainingData = implementIndicators(handleOHLC(ticker))
    else:
        trainingData = implementIndicators(handleOHLC(ticker, startDate = startDate, endDate = endDate))
    nRows, nCols = trainingData.shape
    linearRegModelPreds, linearRegModelCoefs, linearRegModelIntercept, linearRegModelScore = deployLinearModel(*chronologicalSplit(scaleData(trainingData),0.8,nRows))
    randomForestPreds, randomForestWeights, randomForestScore = deployRandomForest(*chronologicalSplit(scaleData(trainingData), 0.8, nRows))
    GBTreePreds, GBTreeWeights, GBTreeScore = deployGBM(*chronologicalSplit(scaleData(trainingData), 0.8, nRows))
    GBLinPreds, GBLinWeights, GBLinScore = deployGBM(*chronologicalSplit(scaleData(trainingData), 0.8, nRows), booster = "gblinear")
    scores = {"Linear Regression" : linearRegModelScore, "Random Forest": randomForestScore,"Gradient Boosting Tree":GBTreeScore,"Gradient Boosting Linear":GBLinScore}
    weights = {"Linear Regression":linearRegModelCoefs,"Random Forest":randomForestWeights,"Gradient Boosting Tree":GBTreeWeights,"Gradient Boosting Linear":GBLinWeights}
    predictions = {"Linear Regression":linearRegModelPreds,"Random Forest":randomForestPreds,"Gradient Boosting Tree":GBTreePreds,"Gradient Boosting Linear":GBLinPreds}
    return scores, weights, predictions

scoreDict, weightsDict, predictionsDict = basicDeploy("JNJ")
