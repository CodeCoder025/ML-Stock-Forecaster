import pandas as pd
import numpy as np
import yfinance as yf
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from xgboost import XGBRegressor

#IMPORTANT REMINDER: I cannot have the model predict today's price change using today's volume --- i can only use the data which I had before today. Prices to predict are close prices, not high,low, or open!
#Columns which has a name starting with "!" are columns which CANNOT be given to the model in predicting prices.
#Idea: Can also one-hot encode for crossovers
#An important concept to note throughout the course of this project is that the model is not able to cross-compare between metrics, thus any possible correlation must be spoon-fed. It is also worth noting that the model cannot put things into context, metrics have to be put into a percentage with little undesirable drift over time. (e.g. if i feed it OHLC< predictions will take after more recent price action, because open, high, low, and close prices now are more like those in 2025 than those in 1995.
#To avoid logical errors, I will call .dropna() as soon as NaN values are present
#Remember not to feed non-contextual data into ML model (e.g. never feed raw OHLC data or raw SMA data, but feed RELATIVE data --- not 38.50 price data but 3% up from last week pricce data. Not 5028000 shares traded on Friday type of volume data, but 14% higher volume as compared to monthly average type of data.
#^^^ The above is obvious because more recent data will have higher everything --- higher price, higher volume, higher volatility etc. Thus make everything in relative percentages so that model will not be affected by magnitude drifts over time.

def handleOHLC(ticker,startDate,endDate):
  startDate = startDate
  endDate = endDate
  ticker = ticker
  OHLC_data = yf.download(ticker, start = startDate, end = endDate).reset_index()
  Emini500 = yf.download("ES=F",start = startDate, end = endDate, interval = "1d").reset_index()
  OHLC_data.columns = OHLC_data.columns.get_level_values(0)
  Emini500.columns = Emini500.columns.get_level_values(0)
  Emini500["Emini_%Spread"] = abs(Emini500["High"]-Emini500["Low"])/((Emini500["Open"]+Emini500["Close"])/2)
  Emini500["Emini_%Change"] = (Emini500["Close"]-Emini500["Open"])/Emini500["Open"]
  Emini500 = Emini500.rename(columns = {"Open" : "Emini_Open",
                                        "Close": "Emini_Close",
                                        "High": "Emini_High",
                                        "Low":"Emini_Low",
                                        "Volume":"Emini_Volume"})
  OHLC_data = pd.merge(OHLC_data, Emini500, on = "Date", how = "inner")
  OHLC_data["!tomorrowPrice"] = OHLC_data["Close"].shift(-1)
  OHLC_data.dropna(inplace = True)
  OHLC_data["%changeTomorrow"] = (OHLC_data["!tomorrowPrice"]-OHLC_data["Close"])/OHLC_data["Close"] #This is the target variable
  return OHLC_data

def xDMA_column(columnName,x, OHLC_data): #it is important to remember that the result returned by this is the RATIO of the metric type to the moving average. It returns a ratio, not a raw/absolute value
    newColumnName = "%sTo%sDay_SMA"%(columnName,x)
    OHLC_data[newColumnName] = None
    newColumnIndex = OHLC_data.columns.get_loc(newColumnName)
    for index, row in OHLC_data.iterrows():
        if index < x:
            continue
        else:
            cumSum = 0
            for i in range(0,x):
                cumSum += OHLC_data.iloc[index-i, OHLC_data.columns.get_loc(columnName)]
            MA = cumSum/x
            OHLC_data.iloc[index,newColumnIndex] = row[columnName]/MA
            
def checkForCrossover(metric1, metric2, rowIndex, OHLC_data,metric1Multiplier = None, metric2Multiplier = None): #OOH i just realised that metricMultipliers will be necessary since most columns in the dataframe are ratio or percentage-based
    #function will return 1 if metric1 crosses over metric 2, -1 if 1 falls below 2 (ie 2 over 1), 0 if neither.
    if metric1Multiplier:
        metric1MultiplierPrev = OHLC_data.iloc[rowIndex-1, OHLC_data.columns.get_loc(metric1Multiplier)]
        metric1MultiplierNow = OHLC_data.iloc[rowIndex, OHLC_data.columns.get_loc(metric1Multiplier)]
    else:
        metric1MultiplierPrev = 1
        metric1MultiplierNow = 1
    if metric2Multiplier:
        metric2MultiplierPrev = OHLC_data.iloc[rowIndex-1, OHLC_data.columns.get_loc(metric2Multiplier)]
        metric2MultiplierNow = OHLC_data.iloc[rowIndex, OHLC_data.columns.get_loc(metric2Multiplier)]
    else:
        metric2MultiplierPrev = 1
        metric2MultiplierNow = 1
    if rowIndex == 0:
        return
    else:
        initialState = 3 #As an indicator of which is initially higher. 3 is if 1 is initially higher (slightly confusing but this is so that i can reduce the number of if statements later on
        if OHLC_data.iloc[rowIndex-1, OHLC_data.columns.get_loc(metric1)]*metric1MultiplierPrev < OHLC_data.iloc[rowIndex-1, OHLC_data.columns.get_loc(metric2)]*metric2MultiplierPrev:
            initialState = 2
        if OHLC_data.iloc[rowIndex, OHLC_data.columns.get_loc(metric1)]*metric1MultiplierNow < OHLC_data.iloc[rowIndex,OHLC_data.columns.get_loc(metric2)]*metric2MultiplierNow:
            return (2-initialState)
        elif OHLC_data.iloc[rowIndex, OHLC_data.columns.get_loc(metric1)]*metric1MultiplierNow > OHLC_data.iloc[rowIndex,OHLC_data.columns.get_loc(metric2)]*metric2MultiplierNow:
            return (3-initialState)    #ngl i really like this maths
        else:
            return 0

def configureDateData(OHLC_data):
    OHLC_data["Date"] = pd.to_datetime(OHLC_data["Date"])
    OHLC_data["Month"] = OHLC_data["Date"].dt.month
    OHLC_data["DayOfMonth"] = OHLC_data["Date"].dt.day
    OHLC_data["Day"] = OHLC_data["Date"].dt.dayofweek

def implementIndicators(OHLC_data):
    for j in range(3,5):
      xDMA_column("Volume",j**2,OHLC_data)
      xDMA_column("Close",j**2,OHLC_data)
      xDMA_column("Emini_Close",j**2,OHLC_data)
    OHLC_data.dropna(inplace=True)
    OHLC_data=OHLC_data.reset_index()
    OHLC_data.drop(columns=["index"], inplace=True)
    configureDateData(OHLC_data)
    
    for j in range(3,5):
      OHLC_data["Price-%sD_crossover"%j**2] = [checkForCrossover("Close","CloseTo%sDay_SMA"%j**2,i, OHLC_data,metric2Multiplier = "Close") for i in range(len(OHLC_data))]
      OHLC_data["Emini-%sD_crossover"%j**2] = [checkForCrossover("Emini_Close","Emini_CloseTo%sDay_SMA"%j**2,i,OHLC_data, metric2Multiplier = "Emini_Close") for i in range(len(OHLC_data))]
    
    OHLC_data.dropna(inplace=True)
    encoder = preprocessing.OneHotEncoder()
    categoricalFeatures = ["Month","Day","DayOfMonth"]
    for feature in categoricalFeatures:
      encoder.fit(OHLC_data[[feature]])
      oneHotOrder = encoder.categories_
      oneHotOrder = ["%s_%s"%(feature, i) for i in oneHotOrder[0]]
      oneHotDF = pd.DataFrame(encoder.transform(OHLC_data[[feature]]).toarray(), columns = oneHotOrder)
      OHLC_data = OHLC_data.reset_index()
      OHLC_data.drop(columns=["index"],inplace=True)
      OHLC_data = pd.merge(OHLC_data, oneHotDF, left_index = True, right_index = True)
    
    trainingData = OHLC_data.drop(columns = ["Date","Close","High","Low","Month","Open","Volume","Emini_Close","Emini_Open","Emini_Volume","Emini_High","Emini_Low","!tomorrowPrice"])
    return trainingData

def scaleData(trainingData):
    minMaxScaler = MinMaxScaler(feature_range=(0,1))
    scaledTrainData = pd.DataFrame(minMaxScaler.fit_transform(trainingData.drop(columns=["%changeTomorrow"])), columns = [i for i in trainingData.columns.to_list() if i != "%changeTomorrow"])
    scaledTrainData["%changeTomorrow"] = trainingData["%changeTomorrow"]
    return scaledTrainData

def chronologicalSplit(scaledTrainData,x):
    nRows, _ = scaledTrainData.shape
    trainingRows = int(x*nRows)
    trainingSet = scaledTrainData.loc[0:trainingRows]
    testSet = scaledTrainData.loc[trainingRows+1:]
    return trainingSet, testSet

def deployLinearModel(trainData, testData):
  linearRegModel = LinearRegression()
  linearRegModel.fit(trainData.drop(columns = ["%changeTomorrow"]), trainData["%changeTomorrow"])
  linearRegModelPreds =  linearRegModel.predict(testData.drop(columns=["%changeTomorrow"]))
  return linearRegModelPreds, linearRegModel.coef_, linearRegModel.intercept_, compute_RMSE(linearRegModelPreds, testData["%changeTomorrow"])

def deployRandomForest(trainData, testData, bootstrapSize = 0.7, n_dTrees = 150, max_leaf_nodes = 200, max_features = "sqrt"):
    model = RandomForestRegressor(n_jobs=-1,
                                  max_samples = bootstrapSize,
                                  n_estimators = n_dTrees,
                                  max_leaf_nodes = max_leaf_nodes,
                                  max_features = max_features)
    model.fit(trainData.drop(columns=["%changeTomorrow"]),
              trainData["%changeTomorrow"])
    randomForestPreds = model.predict(testData.drop(columns=["%changeTomorrow"]))
    return randomForestPreds, model.feature_importances_,compute_RMSE(randomForestPreds, testData["%changeTomorrow"])

def deployGBM(trainData, testData, n_estimators = 100, learning_rate = 0.6, booster = "gbtree", bootstrapSize = 0.7, max_features = "sqrt"):
    if max_features == "sqrt":
        max_features = int((len(trainData.columns.to_list())-1)**0.5)/(len(trainData.columns.to_list())-1)
    GBM = XGBRegressor(n_estimators = n_estimators,
                      learning_rate = learning_rate,
                      booster = booster,
                      subsample = bootstrapSize,
                      colsample_bytree = max_features)
    GBM.fit(trainData.drop(columns=["%changeTomorrow"]), trainData["%changeTomorrow"])
    GBMPreds = GBM.predict(testData.drop(columns=["%changeTomorrow"]))
    if booster == "gbtree":
        weightsDict = GBM.get_booster().get_score(importance_type = "gain")
    elif booster == "gblinear":
        weightsDict = GBM.get_booster().get_score(importance_type = "weight")
    #this one is a bit troublesome because tree-based models may not split trees for every metric provided, and in the case of xgboost if this happens the weight will just be omitted from the output dictionary thus will cause indexing errors later on
    unusedFeatureIndexes = []
    for i in range(len(trainData.drop(columns=["%changeTomorrow"]).columns.to_list())):
        if trainData.drop(columns=["%changeTomorrow"]).columns.to_list()[i] in list(weightsDict.keys()):
            pass
        else:
            unusedFeatureIndexes.append(i)
    unzippedDict = list(weightsDict.items()) #apparently it is impossible to directly insert a key,value pair at a specific index in a dictionary, so ill have to break it down and reconstruct it
    for i in unusedFeatureIndexes:
        unzippedDict.insert(i, (trainData.drop(columns=["%changeTomorrow"]).columns.to_list()[i],0))
    weightsDict = dict(unzippedDict)
    weightsList = [i for i in weightsDict.values()]
    return GBMPreds, weightsList, compute_RMSE(GBMPreds, testData["%changeTomorrow"])

def compute_RMSE(predicted, actual):
    SE = 0
    for i in range(len(predicted)):
        SE += (predicted[i] - actual.iloc[i])**2
    return (SE/len(predicted))**0.5

def RMSEdumb(testData):
    meanPred = np.mean([i for i in testData["%changeTomorrow"]])
    SE = 0
    for i in testData["%changeTomorrow"]:
        SE += abs(i - meanPred)**2
    MSE = SE/len(testData["%changeTomorrow"])
    return MSE**0.5

def basicDeploy(ticker, startDate = "2010-01-01", endDate = "2015-01-01"):
    trainingData = implementIndicators(handleOHLC(ticker, startDate, endDate))
    nRows, nCols = trainingData.shape
    linearRegModelPreds, linearRegModelCoefs, linearRegModelIntercept, linearRegModelScore = deployLinearModel(*chronologicalSplit(scaleData(trainingData),0.8))
    randomForestPreds, randomForestWeights, randomForestScore = deployRandomForest(*chronologicalSplit(scaleData(trainingData), 0.8))
    GBTreePreds, GBTreeWeights, GBTreeScore = deployGBM(*chronologicalSplit(scaleData(trainingData), 0.8))
    GBLinPreds, GBLinWeights, GBLinScore = deployGBM(*chronologicalSplit(scaleData(trainingData), 0.8), booster = "gblinear")
    scores = {"Linear Regression" : linearRegModelScore, "Random Forest": randomForestScore,"Gradient Boosting Tree":GBTreeScore,"Gradient Boosting Linear":GBLinScore}
    weights = {"Linear Regression":linearRegModelCoefs,"Random Forest":randomForestWeights,"Gradient Boosting Tree":GBTreeWeights,"Gradient Boosting Linear":GBLinWeights}
    predictions = {"Linear Regression":linearRegModelPreds,"Random Forest":randomForestPreds,"Gradient Boosting Tree":GBTreePreds,"Gradient Boosting Linear":GBLinPreds}
    _,testData = chronologicalSplit(scaleData(trainingData),0.8)
    return scores, weights, predictions, trainingData, RMSEdumb(testData)

scoreDict, weightsDict, predictionsDict, trainingData, dumbScore = basicDeploy("JNJ")

